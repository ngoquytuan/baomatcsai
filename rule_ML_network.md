# üìä PHASE 1: HI·ªÇU B√ÄI TO√ÅN - NETWORK ANOMALY DETECTION

## Slides Text cho Module 6

---

## SLIDE 1: PHASE 1 OVERVIEW

**HI·ªÇU B√ÄI TO√ÅN - 5 B∆Ø·ªöC QUAN TR·ªåNG**

```
Phase 1: Problem Understanding
‚îú‚îÄ B∆∞·ªõc 1.1: X√°c ƒë·ªãnh lo·∫°i b√†i to√°n ML
‚îú‚îÄ B∆∞·ªõc 1.2: X√°c ƒë·ªãnh success metrics
‚îú‚îÄ B∆∞·ªõc 1.3: Thu th·∫≠p y√™u c·∫ßu k·ªπ thu·∫≠t
‚îú‚îÄ B∆∞·ªõc 1.4: Ph√¢n t√≠ch business context
‚îî‚îÄ B∆∞·ªõc 1.5: Document to√†n b·ªô requirements
```

**Th·ªùi gian:** 2-4 gi·ªù (kh√¥ng b·ªè qua!)

**T·∫°i sao quan tr·ªçng:** 
- Hi·ªÉu sai b√†i to√°n ‚Üí L√†m sai h·∫øt
- Ch·ªçn sai metrics ‚Üí Model "t·ªët" nh∆∞ng v√¥ d·ª•ng
- Thi·∫øu requirements ‚Üí Deploy th·∫•t b·∫°i

---

## SLIDE 2: T√åNH HU·ªêNG TH·ª∞C T·∫æ

**B·∫°n l√† Security Engineer t·∫°i ng√¢n h√†ng ABC**

**Email t·ª´ CTO:**
```
Subject: Urgent - Need AI for Network Security

Ch√∫ng ta ƒëang c√≥ v·∫•n ƒë·ªÅ v·ªõi network security.
M·ªói ng√†y c√≥ h√†ng tri·ªáu network events, team SOC
kh√¥ng th·ªÉ theo d√µi h·∫øt. C·∫ßn AI gi√∫p ph√°t hi·ªán
c√°c anomalies t·ª± ƒë·ªông.

Deadline: 2 th√°ng
Budget: 50,000 USD

C√≥ l√†m ƒë∆∞·ª£c kh√¥ng?
```

**C√¢u h·ªèi:** B·∫°n tr·∫£ l·ªùi th·∫ø n√†o?

---

## SLIDE 3: B∆Ø·ªöC 1.1 - X√ÅC ƒê·ªäNH LO·∫†I B√ÄI TO√ÅN

**C√¢u h·ªèi ph√¢n t√≠ch:**

**Q1: ƒê√¢y l√† b√†i to√°n g√¨?**
- Classification? (ph√¢n lo·∫°i)
- Regression? (d·ª± ƒëo√°n s·ªë)
- Clustering? (ph√¢n nh√≥m)
- Anomaly Detection? (ph√°t hi·ªán b·∫•t th∆∞·ªùng)

**Q2: C√≥ labels kh√¥ng?**
- Supervised learning (c√≥ labels)
- Unsupervised learning (kh√¥ng labels)
- Semi-supervised (m·ªôt ph·∫ßn c√≥ labels)

---

## SLIDE 4: PH√ÇN T√çCH B√ÄI TO√ÅN - NETWORK ANOMALY

**ƒê·∫∑c ƒëi·ªÉm b√†i to√°n:**

‚úÖ **Ph√°t hi·ªán b·∫•t th∆∞·ªùng** (Anomaly Detection)
- M·ª•c ti√™u: T√¨m network events "l·∫°"
- Normal traffic: 99%
- Abnormal traffic: 1%

‚úÖ **Binary Classification** (n·∫øu c√≥ labels)
- Class 0: Normal traffic
- Class 1: Anomaly/Attack

‚úÖ **Highly Imbalanced**
- Anomaly r·∫•t hi·∫øm (1-5%)
- Kh√¥ng th·ªÉ d√πng accuracy ƒë∆°n thu·∫ßn

---

## SLIDE 5: QUY·∫æT ƒê·ªäNH LO·∫†I B√ÄI TO√ÅN

**3 Approaches c√≥ th·ªÉ d√πng:**

**Approach 1: Supervised Classification**
- C·∫ßn: Labeled data (normal + attack)
- Pros: Accuracy cao, bi·∫øt attack types
- Cons: C·∫ßn nhi·ªÅu labeled attack samples
- D√πng khi: C√≥ ƒë·ªß labeled historical data

**Approach 2: Unsupervised Anomaly Detection**
- C·∫ßn: Ch·ªâ c·∫ßn normal traffic data
- Pros: Ph√°t hi·ªán ƒë∆∞·ª£c unknown attacks
- Cons: False positive cao h∆°n
- D√πng khi: √çt labeled data, nhi·ªÅu zero-day

**Approach 3: Semi-supervised**
- C·∫ßn: Nhi·ªÅu normal + √≠t attack samples
- Pros: C√¢n b·∫±ng accuracy v√† flexibility
- Cons: Ph·ª©c t·∫°p h∆°n
- D√πng khi: Dataset th·ª±c t·∫ø (typical case)

---

## SLIDE 6: L·ª∞A CH·ªåN CHO B√ÄI TO√ÅN C·ª¶A CH√öNG TA

**Quy·∫øt ƒë·ªãnh: Supervised Binary Classification**

**L√Ω do:**
‚úÖ C√≥ historical attack logs (labeled)
‚úÖ Bi·∫øt r√µ attack types c·∫ßn detect
‚úÖ Business c·∫ßn gi·∫£i th√≠ch ƒë∆∞·ª£c decisions
‚úÖ C√≥ th·ªÉ train offline tr∆∞·ªõc khi deploy

**Lo·∫°i b√†i to√°n ch√≠nh th·ª©c:**
```
Binary Classification Problem
- Input: Network traffic features
- Output: {0: Normal, 1: Anomaly/Attack}
- Type: Supervised learning
- Challenge: Highly imbalanced data (99:1)
```

---

## SLIDE 7: B∆Ø·ªöC 1.2 - X√ÅC ƒê·ªäNH SUCCESS METRICS

**C√¢u h·ªèi quan tr·ªçng:**
"Model t·ªët" ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a th·∫ø n√†o?

**Metrics c√≥ th·ªÉ d√πng:**
- Accuracy?
- Precision?
- Recall?
- F1-Score?
- AUC-ROC?

**C√¢u h·ªèi:** Metric n√†o quan tr·ªçng nh·∫•t?

---

## SLIDE 8: PH√ÇN T√çCH COST C·ª¶A L·ªñI

**False Positive (FP): Model b√°o attack nh∆∞ng th·ª±c ra normal**

**Cost:**
- Block legitimate traffic
- Business disruption
- IT team ph·∫£i investigate (waste time)
- User complaints

**∆Ø·ªõc t√≠nh:** $500 per false alarm √ó 100 FP/day = $50,000/day ‚ùå

---

## SLIDE 9: PH√ÇN T√çCH COST C·ª¶A L·ªñI (tt)

**False Negative (FN): Model n√≥i normal nh∆∞ng th·ª±c ra l√† attack**

**Cost:**
- Attack kh√¥ng ƒë∆∞·ª£c ph√°t hi·ªán
- Data breach
- Financial loss
- Reputation damage
- Regulatory fines

**∆Ø·ªõc t√≠nh:** $1,000,000 per breach (average) üí•

**K·∫øt lu·∫≠n:** FN nguy hi·ªÉm h∆°n FP (1000x)

---

## SLIDE 10: CH·ªåN PRIMARY METRIC

**D·ª±a tr√™n cost analysis:**

**Primary Metric: RECALL (Sensitivity)**
```
Recall = TP / (TP + FN)
       = "Trong c√°c attack th·∫≠t, b·∫Øt ƒë∆∞·ª£c bao nhi√™u?"
```

**Y√™u c·∫ßu:** Recall ‚â• 95%
- B·ªè s√≥t t·ªëi ƒëa 5% attacks
- Ch·∫•p nh·∫≠n False Positive cao h∆°n

**Secondary Metrics:**
- Precision ‚â• 70% (gi·∫£m FP)
- F1-Score (c√¢n b·∫±ng)
- AUC-ROC ‚â• 0.90

---

## SLIDE 11: ƒê·ªäNH NGHƒ®A SUCCESS CRITERIA

**Model ƒë∆∞·ª£c coi l√† "th√†nh c√¥ng" khi:**

‚úÖ **Performance:**
- Recall ‚â• 95% (must-have)
- Precision ‚â• 70% (nice-to-have)
- F1-Score ‚â• 0.80
- AUC-ROC ‚â• 0.90

‚úÖ **Speed:**
- Inference time < 100ms per event
- Can process 10,000 events/second

‚úÖ **Size:**
- Model size < 100MB
- Deployable on edge devices

---

## SLIDE 12: B∆Ø·ªöC 1.3 - Y√äU C·∫¶U K·ª∏ THU·∫¨T

**Thu th·∫≠p y√™u c·∫ßu t·ª´ stakeholders:**

**From CTO:**
- Deploy trong 2 th√°ng
- Budget: $50K
- Must integrate v·ªõi existing SIEM

**From SOC Team:**
- Real-time alerts
- Explain why it's anomaly
- Dashboard v·ªõi visualizations
- False positive < 100/day

**From IT Ops:**
- High availability (99.9% uptime)
- Scalable (10K events/sec ‚Üí 100K)
- Easy to update/retrain

---

## SLIDE 13: Y√äU C·∫¶U CH·ª®C NƒÇNG

**Functional Requirements:**

**FR1: Detection**
- [ ] Detect DDoS attacks
- [ ] Detect port scanning
- [ ] Detect botnet traffic
- [ ] Detect data exfiltration
- [ ] Detect insider threats

**FR2: Alert System**
- [ ] Real-time alerts (<1 second)
- [ ] Severity levels (Low/Medium/High/Critical)
- [ ] Integration v·ªõi Slack/Email/SMS
- [ ] Incident ticket creation

**FR3: Dashboard**
- [ ] Traffic overview
- [ ] Attack statistics
- [ ] Model performance metrics
- [ ] Feature importance

---

## SLIDE 14: Y√äU C·∫¶U PHI CH·ª®C NƒÇNG

**Non-Functional Requirements:**

**NFR1: Performance**
```
Throughput:    ‚â• 10,000 events/second
Latency:       < 100ms (p95)
Model size:    < 100MB
Memory usage:  < 4GB RAM
```

**NFR2: Reliability**
```
Uptime:        99.9% (8.76 hours downtime/year)
MTBF:          > 720 hours
MTTR:          < 1 hour
Backup:        Daily automated
```

**NFR3: Scalability**
```
Horizontal:    Support 10+ nodes
Vertical:      Up to 32 cores, 128GB RAM
Auto-scaling:  Yes, based on load
```

---

## SLIDE 15: Y√äU C·∫¶U PHI CH·ª®C NƒÇNG (tt)

**NFR4: Security**
```
Authentication:  RBAC with AD integration
Encryption:      TLS 1.3 in-transit, AES-256 at-rest
Audit logging:   All predictions logged
Compliance:      SOC 2, ISO 27001
```

**NFR5: Maintainability**
```
Monitoring:      Prometheus + Grafana
Logging:         ELK stack
CI/CD:           Jenkins pipeline
Documentation:   Comprehensive docs + runbooks
```

**NFR6: Usability**
```
Training time:   < 4 hours for SOC analysts
UI/UX:           Intuitive dashboard
Explainability:  LIME/SHAP for predictions
```

---

## SLIDE 16: B∆Ø·ªöC 1.4 - BUSINESS CONTEXT

**Hi·ªÉu r√µ business domain:**

**Industry:** Financial Services (Banking)
- Highly regulated (PCI-DSS, GDPR)
- 24/7 operations
- Zero-tolerance for breaches
- Customer trust critical

**Current State:**
- Manual SOC monitoring
- 3 analysts √ó 8-hour shifts
- React to alerts (not proactive)
- Miss ~30% of attacks

**Target State:**
- AI-assisted detection
- Proactive threat hunting
- Reduce analyst workload 80%
- Catch 95%+ of attacks

---

## SLIDE 17: STAKEHOLDER ANALYSIS

**Primary Stakeholders:**

**1. SOC Team (Users)**
- Need: Easy-to-use tools
- Pain: Alert fatigue (500+ alerts/day)
- Success: Reduce alerts to 50/day
- Involvement: Daily users, feedback

**2. CTO (Sponsor)**
- Need: ROI proof
- Pain: Recent breaches ($2M loss)
- Success: No breaches in 6 months
- Involvement: Budget approval, reviews

**3. Compliance Officer**
- Need: Audit trail
- Pain: Regulatory fines risk
- Success: Pass audits
- Involvement: Compliance checks

---

## SLIDE 18: STAKEHOLDER ANALYSIS (tt)

**4. IT Operations (Support)**
- Need: Reliable system
- Pain: Downtime impacts business
- Success: 99.9% uptime
- Involvement: Deployment, maintenance

**5. Network Engineers (Data Source)**
- Need: Non-intrusive monitoring
- Pain: Tools slow down network
- Success: <1% performance impact
- Involvement: Data access, integration

**6. CISO (Decision Maker)**
- Need: Risk reduction
- Pain: Board pressure
- Success: Measurable security improvement
- Involvement: Final approval

---

## SLIDE 19: CONSTRAINTS & ASSUMPTIONS

**Constraints (Limitations):**

**Technical:**
- Must use existing network infrastructure
- Cannot install agents on endpoints
- Data retention: 90 days only
- Processing: On-premise only (no cloud)

**Resource:**
- Budget: $50K (including licenses)
- Team: 1 ML engineer, 1 DevOps
- Timeline: 2 months to MVP
- Compute: 2 servers (32 cores, 128GB each)

**Regulatory:**
- PCI-DSS Level 1 compliance
- GDPR data protection
- No PII in logs
- Audit logs required

---

## SLIDE 20: CONSTRAINTS & ASSUMPTIONS (tt)

**Assumptions (Need validation):**

‚úì Network logs are complete and accurate
‚úì Attack labels in historical data are correct
‚úì Network topology stable (no major changes)
‚úì SOC team will provide feedback
‚úì IT will support integration
‚úì Data quality is sufficient

**Risk if assumptions wrong:**
- Incomplete logs ‚Üí Poor model
- Wrong labels ‚Üí Train on bad data
- Topology changes ‚Üí Model outdated
- No feedback ‚Üí Can't improve

---

## SLIDE 21: B∆Ø·ªöC 1.5 - DOCUMENT REQUIREMENTS

**T·∫°o Requirements Document:**

**1. Problem Statement (1 page)**
```
Background:
- Current state
- Pain points
- Business impact

Proposed Solution:
- ML-based anomaly detection
- Real-time alerts
- Dashboard

Expected Outcomes:
- 95% detection rate
- 80% workload reduction
- ROI within 6 months
```

---

## SLIDE 22: DOCUMENT REQUIREMENTS (tt)

**2. Technical Specifications (2-3 pages)**
```
Input:
- Data source: Network flow logs (NetFlow, sFlow)
- Format: CSV/JSON
- Volume: 1M events/hour
- Features: 50+ fields

Output:
- Prediction: {0: Normal, 1: Anomaly}
- Confidence score: 0-100%
- Explanation: Top 3 contributing factors
- Alert: JSON to SIEM

Model:
- Type: Binary classifier
- Algorithms: Compare 5 (LR, RF, XGBoost, NN, Isolation Forest)
- Training: Weekly retrain
- Validation: 95% recall minimum
```

---

## SLIDE 23: DOCUMENT REQUIREMENTS (tt)

**3. Success Metrics (1 page)**
```
Primary (Must-Have):
‚úì Recall ‚â• 95%
‚úì Inference < 100ms
‚úì Uptime ‚â• 99.9%

Secondary (Nice-to-Have):
‚úì Precision ‚â• 70%
‚úì F1-Score ‚â• 0.80
‚úì False alerts < 100/day

Business Metrics:
‚úì Breach reduction: 80%
‚úì SOC efficiency: 80% improvement
‚úì Cost savings: $500K/year
‚úì ROI: 6 months payback
```

---

## SLIDE 24: DOCUMENT REQUIREMENTS (tt)

**4. Risks & Mitigation (1 page)**

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Insufficient labeled data | High | Medium | Use semi-supervised learning |
| High false positive rate | High | High | Tune threshold, ensemble methods |
| Concept drift | Medium | High | Weekly retraining pipeline |
| Integration issues | Medium | Low | POC with IT before full deploy |
| Performance degradation | High | Low | Load testing, auto-scaling |

---

## SLIDE 25: REVIEW CHECKLIST

**Phase 1 Complete khi:**

‚úÖ **Problem Definition:**
- [ ] Lo·∫°i b√†i to√°n x√°c ƒë·ªãnh r√µ r√†ng
- [ ] Input v√† output ƒë∆∞·ª£c m√¥ t·∫£ c·ª• th·ªÉ
- [ ] Supervised vs unsupervised ƒë√£ quy·∫øt ƒë·ªãnh

‚úÖ **Success Metrics:**
- [ ] Primary metric ƒë√£ ch·ªçn (Recall)
- [ ] Secondary metrics ƒë√£ list
- [ ] Threshold requirements ƒë√£ set
- [ ] Business metrics ƒë√£ align

‚úÖ **Requirements:**
- [ ] Functional requirements documented
- [ ] Non-functional requirements documented
- [ ] Constraints identified
- [ ] Assumptions listed

---

## SLIDE 26: REVIEW CHECKLIST (tt)

‚úÖ **Stakeholders:**
- [ ] All stakeholders identified
- [ ] Needs and pains understood
- [ ] Success criteria agreed
- [ ] Sign-off obtained

‚úÖ **Documentation:**
- [ ] Requirements doc created
- [ ] Reviewed by stakeholders
- [ ] Approved by sponsor (CTO)
- [ ] Shared with team

‚úÖ **Next Steps:**
- [ ] Phase 2: Data collection plan ready
- [ ] Team briefed
- [ ] Timeline confirmed

---

## SLIDE 27: COMMON MISTAKES - TR√ÅNH SAI L·∫¶M

**‚ùå Mistake 1: B·ªè qua Phase 1**
```
"M√¨nh c√≥ data r·ªìi, train model lu√¥n ƒëi!"
‚Üí L√†m sai b√†i to√°n, t·ªën th·ªùi gian

ƒê√∫ng: D√†nh 10% th·ªùi gian hi·ªÉu b√†i to√°n
      Ti·∫øt ki·ªám 50% th·ªùi gian sau n√†y
```

**‚ùå Mistake 2: Ch·ªâ nh√¨n accuracy**
```
"Model ƒë·∫°t 99% accuracy r·ªìi!"
‚Üí Nh∆∞ng b·ªè s√≥t 50% attacks (v√¨ imbalanced)

ƒê√∫ng: Ch·ªçn metric ph√π h·ª£p (Recall cho security)
```

---

## SLIDE 28: COMMON MISTAKES (tt)

**‚ùå Mistake 3: Kh√¥ng h·ªèi stakeholders**
```
ML engineer t·ª± quy·∫øt ƒë·ªãnh requirements
‚Üí Deploy xong, users kh√¥ng d√πng

ƒê√∫ng: Interview users, hi·ªÉu pain points th·∫≠t
```

**‚ùå Mistake 4: Unrealistic expectations**
```
"AI s·∫Ω detect 100% attacks, 0% false positive"
‚Üí Impossible, stakeholders th·∫•t v·ªçng

ƒê√∫ng: Set realistic goals (95% recall, 70% precision)
      Over-deliver thay v√¨ under-deliver
```

---

## SLIDE 29: COMMON MISTAKES (tt)

**‚ùå Mistake 5: Thi·∫øu documentation**
```
Nh·ªõ trong ƒë·∫ßu, kh√¥ng vi·∫øt ra
‚Üí 2 th√°ng sau qu√™n, team m·ªõi kh√¥ng hi·ªÉu

ƒê√∫ng: Document everything
      Future you s·∫Ω c·∫£m ∆°n present you
```

**‚ùå Mistake 6: Ignore constraints**
```
Train model c·∫ßn 1TB RAM, 10 GPUs
‚Üí C√¥ng ty kh√¥ng c√≥, deploy th·∫•t b·∫°i

ƒê√∫ng: Hi·ªÉu constraints t·ª´ ƒë·∫ßu
      Design solution ph√π h·ª£p
```

---

## SLIDE 30: OUTPUT C·ª¶A PHASE 1

**Deliverables:**

üìÑ **1. Requirements Document (5-10 pages)**
- Problem statement
- Technical specifications
- Success metrics
- Risks & mitigation

üìä **2. Project Charter (1 page)**
- Objective
- Scope
- Timeline
- Team
- Budget

üìã **3. Stakeholder Sign-off**
- CTO approval
- SOC manager approval
- IT ops approval

---

## SLIDE 31: OUTPUT C·ª¶A PHASE 1 (tt)

üìà **4. Success Metrics Dashboard (mockup)**
- Show how metrics will be tracked
- Set baseline (current state)
- Define targets

üóìÔ∏è **5. Project Plan**
```
Phase 1: Requirements     [DONE] ‚úÖ
Phase 2: Data Collection  [Week 1-2]
Phase 3: EDA              [Week 3]
Phase 4: Feature Eng      [Week 4]
Phase 5: Modeling         [Week 5-6]
Phase 6: Evaluation       [Week 7]
Phase 7: Deployment       [Week 8]
Phase 8: Monitoring       [Ongoing]
```

---

## SLIDE 32: T√ìM T·∫ÆT PHASE 1

**5 B∆∞·ªõc ƒë√£ th·ª±c hi·ªán:**

‚úÖ **B∆∞·ªõc 1.1:** X√°c ƒë·ªãnh lo·∫°i b√†i to√°n
- Binary classification
- Supervised learning
- Highly imbalanced

‚úÖ **B∆∞·ªõc 1.2:** X√°c ƒë·ªãnh success metrics
- Primary: Recall ‚â• 95%
- Secondary: Precision ‚â• 70%
- Business: Cost savings $500K/year

‚úÖ **B∆∞·ªõc 1.3:** Thu th·∫≠p requirements
- Functional: Detection types
- Non-functional: Performance, reliability

---

## SLIDE 33: T√ìM T·∫ÆT PHASE 1 (tt)

‚úÖ **B∆∞·ªõc 1.4:** Ph√¢n t√≠ch business context
- Stakeholders identified
- Pain points understood
- Constraints documented

‚úÖ **B∆∞·ªõc 1.5:** Document requirements
- Requirements doc created
- Sign-off obtained
- Team aligned

**Th·ªùi gian:** 2-4 gi·ªù well spent!

**K·∫øt qu·∫£:** Clear direction, aligned expectations, reduced risks

---

## SLIDE 34: TEMPLATE - PROBLEM DEFINITION

**Use this template for any ML project:**

```
1. WHAT?
   - What is the problem?
   - What is the goal?
   - What is success?

2. WHY?
   - Why is this important?
   - Why now?
   - Why ML (not rule-based)?

3. WHO?
   - Who are the users?
   - Who are the stakeholders?
   - Who will maintain it?

4. WHEN?
   - When is the deadline?
   - When will it be deployed?
   - When to retrain?

5. WHERE?
   - Where will it run? (cloud/on-premise)
   - Where is the data?
   - Where are the constraints?

6. HOW?
   - How will success be measured?
   - How will it integrate?
   - How will it be monitored?
```

---

## SLIDE 35: REAL-WORLD EXAMPLE 1

**Case: E-commerce Fraud Detection**

**Problem:**
- Fraud transactions: 0.1%
- Loss: $10M/year
- Manual review: Too slow

**Analysis:**
- Type: Binary classification (fraud/legitimate)
- Primary metric: Recall (catch fraudsters)
- Secondary: Precision (reduce false declines)
- Constraint: Real-time (<200ms)
- Success: 90% fraud caught, <5% false declines

---

## SLIDE 36: REAL-WORLD EXAMPLE 2

**Case: Medical Diagnosis (Cancer Detection)**

**Problem:**
- Radiologist shortage
- Need faster screening
- High stakes (life/death)

**Analysis:**
- Type: Binary classification (cancer/no cancer)
- Primary metric: Recall 99%+ (cannot miss cancer)
- Secondary: Precision 80%+ (reduce unnecessary biopsies)
- Constraint: Explainable (doctors need trust)
- Success: FDA approval, hospital adoption

---

## SLIDE 37: REAL-WORLD EXAMPLE 3

**Case: Predictive Maintenance (Factory)**

**Problem:**
- Unexpected equipment failures
- Downtime: $100K/hour
- Reactive maintenance expensive

**Analysis:**
- Type: Binary classification (will fail/won't fail)
- OR: Regression (remaining useful life prediction)
- Primary metric: Recall 95% (catch failures early)
- Constraint: Edge deployment (no cloud access)
- Success: 50% downtime reduction

---

## SLIDE 38: EXERCISE 1 - PH√ÇN T√çCH B√ÄI TO√ÅN

**Scenario:** Spam Email Detection

**Y√™u c·∫ßu:** X√°c ƒë·ªãnh c√°c th√¥ng tin sau:
1. Lo·∫°i b√†i to√°n?
2. Primary metric?
3. False Positive vs False Negative: C√°i n√†o nguy hi·ªÉm h∆°n?
4. Requirements ch√≠nh?

**Th·ªùi gian:** 10 ph√∫t th·∫£o lu·∫≠n nh√≥m

---

## SLIDE 39: EXERCISE 2 - CH·ªåN METRICS

**Cho c√°c b√†i to√°n sau, ch·ªçn primary metric:**

**1. Credit Card Approval**
- Approve good customers
- Reject risky customers
- Primary metric: ?

**2. Disease Outbreak Detection**
- Detect outbreak early
- Avoid panic (false alarms)
- Primary metric: ?

**3. Product Recommendation**
- User clicks on recommendations
- Maximize revenue
- Primary metric: ?

---

## SLIDE 40: ƒê√ÅP √ÅN EXERCISES

**Exercise 1: Spam Detection**
1. Binary classification (spam/not spam)
2. Primary: Precision (kh√¥ng x√≥a nh·∫ßm email quan tr·ªçng)
3. FP nguy hi·ªÉm h∆°n (m·∫•t email business critical)
4. Real-time, low FP, explainable

**Exercise 2: Metrics**
1. Credit Card: F1-Score (balance risk & opportunity)
2. Disease Outbreak: Recall (cannot miss outbreaks)
3. Product Recommendation: Precision@K, CTR

---

## SLIDE 41: BEST PRACTICES SUMMARY

**‚úÖ DO:**
- Spend 10% th·ªùi gian ·ªü Phase 1
- Interview real users
- Document everything
- Set realistic expectations
- Align with business goals
- Get stakeholder sign-off

**‚ùå DON'T:**
- Skip to coding immediately
- Assume you know the problem
- Choose metrics arbitrarily
- Ignore constraints
- Work in isolation
- Promise unrealistic results

---

## SLIDE 42: PHASE 1 CHECKLIST

**Print v√† check off:**

```
‚ñ° Lo·∫°i b√†i to√°n x√°c ƒë·ªãnh
‚ñ° Input/Output m√¥ t·∫£ r√µ
‚ñ° Primary metric ch·ªçn
‚ñ° Secondary metrics list
‚ñ° Threshold requirements set
‚ñ° Stakeholders interviewed
‚ñ° Requirements documented
‚ñ° Constraints identified
‚ñ° Assumptions listed
‚ñ° Risks assessed
‚ñ° Timeline agreed
‚ñ° Budget confirmed
‚ñ° Sign-off obtained
‚ñ° Team briefed
‚ñ° Ready for Phase 2
```

---

## SLIDE 43: CHUY·ªÇN SANG PHASE 2

**Phase 1 ho√†n th√†nh ‚Üí Phase 2: Data Collection**

**C√≥ trong tay:**
- Requirements document
- Success metrics defined
- Stakeholder buy-in
- Clear direction

**Phase 2 s·∫Ω l√†m g√¨:**
- Thu th·∫≠p network logs
- Exploratory Data Analysis
- Data quality assessment
- Feature identification

**Chu·∫©n b·ªã:**
- Access to network logs
- Analysis tools ready
- Team availability

---

## SLIDE 44: Q&A - COMMON QUESTIONS

**Q: Phase 1 c√≥ th·ªÉ skip n·∫øu b√†i to√°n r√µ r√†ng?**
A: KH√îNG! Lu√¥n l√†m Phase 1. "R√µ r√†ng" th∆∞·ªùng l√†Ï∞©illusion.

**Q: M·∫•t bao l√¢u cho Phase 1?**
A: 2-4 gi·ªù cho small project, 1-2 ng√†y cho large project.

**Q: N·∫øu stakeholder kh√¥ng available?**
A: Document assumptions, flag risks, proceed v·ªõi caution.

**Q: Metrics c√≥ th·ªÉ thay ƒë·ªïi sau?**
A: C√≥, nh∆∞ng c·∫ßn justify v√† get approval.

---

## SLIDE 45: T√ÄI LI·ªÜU THAM KH·∫¢O

**Books:**
- "Machine Learning Yearning" - Andrew Ng
- "Building Machine Learning Powered Applications" - Emmanuel Ameisen

**Templates:**
- ML Canvas: https://www.louisdorard.com/ml-canvas
- Project Charter template

**Tools:**
- JIRA/Trello: Project tracking
- Confluence: Documentation
- Google Docs: Collaborative editing

---

## SLIDE 46: HOMEWORK

**B√†i t·∫≠p v·ªÅ nh√†:**

**1. Ph√¢n t√≠ch b√†i to√°n m·ªõi (B·∫Øt bu·ªôc)**
- Ch·ªçn 1 b√†i to√°n ML b·∫•t k·ª≥
- Apply Phase 1 framework
- Vi·∫øt requirements document (2-3 pages)
- N·ªôp tu·∫ßn sau

**2. Critique existing project (N√¢ng cao)**
- T√¨m 1 ML project th·∫•t b·∫°i (news, blog)
- Ph√¢n t√≠ch: C√≥ th·ªÉ h·ªç b·ªè qua Phase 1 kh√¥ng?
- Present findings (5 ph√∫t)

---

## SLIDE 47: KEY TAKEAWAYS

**3 ƒëi·ªÅu quan tr·ªçng nh·∫•t:**

1. **Hi·ªÉu b√†i to√°n > Thu·∫≠t to√°n fancy**
   - 10% th·ªùi gian Phase 1
   - 50% th·ªùi gian ti·∫øt ki·ªám sau

2. **Metrics ph·∫£i align v·ªõi business**
   - Accuracy kh√¥ng ph·∫£i l√∫c n√†o c≈©ng ƒë√∫ng
   - Ch·ªçn metrics theo cost analysis

3. **Documentation is your friend**
   - Requirements doc = roadmap
   - Prevent scope creep
   - Enable communication

---

## SLIDE 48: REMEMBER

> "Hours spent in reconnaissance are seldom wasted."
> - Military proverb

> "Give me six hours to chop down a tree,  
> I will spend the first four sharpening the axe."
> - Abraham Lincoln

**Applied to ML:**
> "Give me two months to build ML system,  
> I will spend first week understanding the problem."

**Phase 1 = Sharpening your axe! ü™ì**

---

## SLIDE 49: NEXT CLASS PREVIEW

**Phase 2: Data Collection & EDA**

**Topics:**
- Network log formats (NetFlow, sFlow)
- Data collection strategies
- EDA techniques for network data
- Data quality assessment
- Feature identification

**Chu·∫©n b·ªã:**
- Review network protocols
- Install Wireshark (optional)
- Read v·ªÅ NetFlow format

